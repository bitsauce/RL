{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from skimage import transform\n",
    "from IPython.display import display, clear_output\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import FrameStack, Scheduler, calculate_expected_return\n",
    "from a2c import GaussianA2C\n",
    "from vec_env.subproc_vec_env import SubprocVecEnv\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    frame = frame[:-12, 6:-6] # Crop to 84x84\n",
    "    frame = np.dot(frame[..., 0:3], [0.299, 0.587, 0.114])\n",
    "    frame = frame / 255.0\n",
    "    frame = frame * 2 - 1\n",
    "    return frame\n",
    "\n",
    "def make_env():\n",
    "    return gym.make(\"CarRacing-v0\")\n",
    "\n",
    "def evaluate(test_env, num_steps=None):\n",
    "    initial_frame = test_env.reset()\n",
    "    frame_stack = FrameStack(initial_frame, preprocess_fn=preprocess_frame)\n",
    "    done = False\n",
    "    step = 0\n",
    "    while not done:\n",
    "        if num_steps is not None and step > num_steps: break\n",
    "        step += 1\n",
    "        # Predict action given state: π(a_t | s_t; θ)\n",
    "        state = frame_stack.get_state()\n",
    "        action, _ = a2c_model.predict(np.expand_dims(state, axis=0))\n",
    "        clear_output(wait=True)\n",
    "        #print(\"Mean:\",action_mean,\"Std:\",action_std)\n",
    "        #action = np.random.normal(loc=actions_mean[0], scale=actions_std[0])\n",
    "        frame, reward, done, info = test_env.step(action[0])\n",
    "        test_env.render()\n",
    "        frame_stack.add_frame(frame)\n",
    "        time.sleep(0.016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_envs = 4\n",
    "envs = SubprocVecEnv([make_env for _ in range(num_envs)])\n",
    "test_env = gym.make(\"CarRacing-v0\")\n",
    "\n",
    "lr_scheduler     = Scheduler(initial_value=1e-6, interval=10, decay_factor=1)#0.95)\n",
    "action_scheduler = Scheduler(initial_value=20, interval=20, decay_factor=0.90)\n",
    "\n",
    "discount_factor  = 0.95\n",
    "save_interval    = 50\n",
    "t_max            = 5\n",
    "frame_stack_size = 4\n",
    "input_shape      = (84, 84, 4)\n",
    "num_actions = envs.action_space.shape[0]\n",
    "action_min = np.array([-1.0, 0.0, 0.0])\n",
    "action_max = np.array([ 1.0, 1.0, 1.0])\n",
    "episode = 0\n",
    "model_checkpoint = None #\"./models/CarRacing-v0/run5/step399942.ckpt\"\n",
    "a2c_model = GaussianA2C(num_actions, input_shape, tf.train.RMSPropOptimizer, action_min, action_max,\n",
    "                        value_scale=0.5, entropy_scale=0.01, model_checkpoint=model_checkpoint, model_name=\"CarRacing-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    print(\"Resetting envronments...\")\n",
    "    episode += 1\n",
    "    T = 0\n",
    "    \n",
    "    # Reset environments and get initial frame\n",
    "    envs.reset()\n",
    "    envs.get_images()\n",
    "    for _ in range(100):\n",
    "        envs.step_async(np.zeros((num_envs, num_actions)))\n",
    "        initial_frames, rewards, dones, infos = envs.step_wait()\n",
    "    frame_stacks = [FrameStack(initial_frames[i], preprocess_fn=preprocess_frame) for i in range(num_envs)]\n",
    "    learning_rate = lr_scheduler.get_value()\n",
    "    action_interval = np.ceil(action_scheduler.get_value())\n",
    "    action_step = 0\n",
    "    total_reward = 0\n",
    "    episode_loss = episode_policy_loss = episode_value_loss = episode_entropy_loss = 0\n",
    "    average_episode_reward = []\n",
    "    \n",
    "    # While there are running environments\n",
    "    print(\"Training...\")\n",
    "    dones = [False] * num_envs\n",
    "    while sum(dones) < num_envs and T < 2000:\n",
    "        states_mb, actions_mb, returns_mb, values_mb = [], [], [], []\n",
    "        \n",
    "        # Simulate game for some number of steps\n",
    "        rewards_mb = []\n",
    "        for _ in range(t_max):\n",
    "            states = [frame_stacks[i].get_state() if dones[i] == False else np.zeros(input_shape) for i in range(num_envs)]\n",
    "            if action_step % action_interval == 0:\n",
    "                # Predict and value action given state\n",
    "                # π(a_t | s_t; θ)\n",
    "                actions, values = a2c_model.predict(states)\n",
    "            else:\n",
    "                _, values = a2c_model.predict(states)\n",
    "            action_step += 1\n",
    "            \n",
    "            for i in range(num_envs):\n",
    "                if np.any(actions[i] < action_min) or np.any(actions[i] > action_max):\n",
    "                    print(\"Something's wrong\")\n",
    "                    print(actions[i])\n",
    "            \n",
    "            # Sample action from a Gaussian distribution\n",
    "            #actions = np.random.normal(loc=actions_mean, scale=actions_std)\n",
    "            envs.step_async(actions)\n",
    "            frames, rewards, dones, infos = envs.step_wait()\n",
    "            rewards = np.array(rewards)\n",
    "            envs.get_images() # render\n",
    "            \n",
    "            # Store state, action and reward\n",
    "            states_mb.append(states)\n",
    "            actions_mb.append(actions)\n",
    "            rewards_mb.append(rewards)\n",
    "            values_mb.append(np.squeeze(values, axis=-1))\n",
    "            total_reward += np.sum(rewards)\n",
    "            \n",
    "            # Get new state\n",
    "            for i in range(num_envs):\n",
    "                frame_stacks[i].add_frame(frames[i])\n",
    "\n",
    "        # Calculate return (discounted rewards over a trajectory)\n",
    "        states = [frame_stacks[i].get_state() if dones[i] == False else np.zeros(input_shape) for i in range(num_envs)]\n",
    "        last_values = a2c_model.predict(states)[-1]\n",
    "        rewards_mb = np.array(rewards_mb)\n",
    "        for i in range(num_envs):\n",
    "            if dones[i] == False:\n",
    "                returns_mb.append(calculate_expected_return(np.append(rewards_mb[:, i], last_values[i]), discount_factor)[:-1])\n",
    "            else:\n",
    "                returns_mb.append(calculate_expected_return(np.append(rewards_mb[:, i], 0), discount_factor)[:-1])\n",
    "   \n",
    "        states_mb = np.array(states_mb).reshape((-1, *input_shape))\n",
    "        actions_mb = np.array(actions_mb).reshape((-1, envs.action_space.shape[0]))\n",
    "        values_mb = np.array(values_mb).flatten()\n",
    "        returns_mb = np.array(returns_mb).transpose(1, 0).flatten()\n",
    "        \n",
    "        eploss, pgloss, vloss, entloss = a2c_model.train(states_mb, actions_mb, returns_mb, values_mb, learning_rate=learning_rate)\n",
    "        episode_loss         += eploss\n",
    "        episode_policy_loss  += pgloss\n",
    "        episode_value_loss   += vloss\n",
    "        episode_entropy_loss += entloss\n",
    "        T += 1\n",
    "    average_episode_reward = total_reward / num_envs\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(\"-- Episode {} --\".format(episode))\n",
    "    print(\"Learning rate:\", learning_rate)\n",
    "    print(\"Episode policy loss:\", episode_policy_loss)\n",
    "    print(\"Episode value loss:\", episode_value_loss)\n",
    "    print(\"Episode entropy loss:\", episode_entropy_loss)\n",
    "    print(\"Episode loss:\", episode_loss)\n",
    "    print(\"Average episode reward:\", average_episode_reward)\n",
    "    print(\"\")\n",
    "    a2c_model.write_summary(episode_policy_loss, episode_value_loss,\n",
    "                            episode_entropy_loss, episode_loss,\n",
    "                            average_episode_reward, learning_rate)\n",
    "    if episode % save_interval == 0:\n",
    "        a2c_model.save()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate(test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a2c_model = GaussianA2C(3, (84, 84, 4), tf.train.RMSPropOptimizer,\n",
    "                        value_scale=0.5, entropy_scale=0.01,\n",
    "                        model_checkpoint=\"models/CarRacing-v0/run6/step40000.ckpt\",\n",
    "                        model_name=\"CarRacing-v0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
