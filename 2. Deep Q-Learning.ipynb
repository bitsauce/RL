{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import vizdoom\n",
    "import os\n",
    "import time\n",
    "import keras\n",
    "import random\n",
    "from keras.layers import Input, Conv2D, Dense, Flatten, MaxPooling2D, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from skimage import transform\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = vizdoom.DoomGame()\n",
    "game.load_config(\"doom/basic.cfg\")\n",
    "#game.set_window_visible(True)\n",
    "game.set_doom_scenario_path(\"doom/basic.wad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes       = 500\n",
    "num_steps          = 100\n",
    "replay_buffer_size = 1000000\n",
    "learning_rate      = 0.0002\n",
    "discount_factor    = 0.95\n",
    "batch_size         = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 84, 84, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 82, 82, 32)   1184        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 41, 41, 32)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 39, 39, 64)   18496       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 19, 19, 64)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 17, 17, 128)  73856       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 36992)        0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          18940416    flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3)            387         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            0           dense_3[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 19,100,003\n",
      "Trainable params: 19,100,003\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_images = Input(shape=(84, 84, 4), dtype=\"float32\")\n",
    "input_action = Input(shape=(3,), dtype=\"float32\") # one-hot vector\n",
    "x = Conv2D(32, (3, 3), activation='elu', padding=\"valid\", input_shape=(84, 84, 4))(input_images)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(64, (3, 3), activation='elu', padding=\"valid\")(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(128, (3, 3), activation='elu', padding=\"valid\")(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(512, activation='elu')(x)\n",
    "x = Dense(128, activation='elu')(x)\n",
    "Q_actions = Dense(game.get_available_buttons_size(), activation=None)(x)\n",
    "Q_input_action = Lambda(lambda x: K.expand_dims(K.sum(x[0] * x[1], axis=1), axis=-1))([Q_actions, input_action]) # Get Q-predicted for input_action\n",
    "\n",
    "training_model = Model(inputs=[input_images, input_action], outputs=[Q_input_action])\n",
    "training_model.compile(loss=\"mse\", optimizer=SGD(lr=learning_rate))\n",
    "training_model.summary()\n",
    "prediction_model = Model(inputs=[input_images], outputs=[Q_actions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    # Crop the screen (remove the roof because it contains no information)\n",
    "    cropped_frame = frame[30:-10, 30:-30]\n",
    "    \n",
    "    # Normalize Pixel Values\n",
    "    normalized_frame = cropped_frame / 255.0\n",
    "    \n",
    "    # Resize\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [84, 84])\n",
    "    \n",
    "    return preprocessed_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-- Episode 499/500 --'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Loss: 11.742753982543945'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "\n",
    "game.close()\n",
    "\n",
    "game.set_episode_timeout(num_steps)\n",
    "game.init()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    game.new_episode()\n",
    "    \n",
    "    # Init frame stack\n",
    "    frame_stack = deque(maxlen=4)\n",
    "    initial_frame = preprocess_frame(game.get_state().screen_buffer)\n",
    "    for _ in range(4):\n",
    "        frame_stack.append(initial_frame)\n",
    "    state = np.stack(frame_stack, axis=2)\n",
    "    \n",
    "    done = False\n",
    "    for step in range(num_steps):\n",
    "        if not done:\n",
    "            # Get action with highest Q-value for current state\n",
    "            action = np.argmax(prediction_model.predict_on_batch(np.expand_dims(state, axis=0)))\n",
    "            action_one_hot = [False] * 3\n",
    "            action_one_hot[action] = True\n",
    "\n",
    "            # Take action and get reward\n",
    "            reward = game.make_action(action_one_hot)\n",
    "            done = game.is_episode_finished()\n",
    "\n",
    "            if not done:\n",
    "                # Get new state\n",
    "                frame_stack.append(preprocess_frame(game.get_state().screen_buffer))\n",
    "                new_state = np.stack(frame_stack, axis=2)\n",
    "            else:\n",
    "                new_state = None\n",
    "\n",
    "            # Store experience\n",
    "            replay_buffer.append((state, action_one_hot, reward, new_state))\n",
    "            state = new_state\n",
    "        \n",
    "        # Train network on expreiences\n",
    "        loss = 0\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            # Get batch\n",
    "            replay_batch      = random.sample(replay_buffer, batch_size)\n",
    "            replay_state      = [r[0] for r in replay_batch]\n",
    "            replay_action     = [r[1] for r in replay_batch]\n",
    "            replay_reward     = [r[2] for r in replay_batch]\n",
    "            replay_next_state = [r[3] for r in replay_batch]\n",
    "            \n",
    "            # Q_target = reward + gamma * max_a' Q(s')\n",
    "            Q_target = []\n",
    "            for i in range(batch_size):\n",
    "                if replay_next_state[i] is not None:\n",
    "                    Q_next_state = prediction_model.predict_on_batch(np.expand_dims(replay_next_state[i], axis=0))[0]\n",
    "                    Q_next_max   = np.max(Q_next_state)\n",
    "                    Q_target.append(replay_reward[i] + discount_factor * Q_next_max)\n",
    "                else:\n",
    "                    Q_target.append(replay_reward[i])\n",
    "            loss += training_model.train_on_batch([replay_state, replay_action], Q_target)\n",
    "            \n",
    "    clear_output(wait=True)\n",
    "    display(\"-- Episode {}/{} --\".format(episode, num_episodes))\n",
    "    display(\"Loss: {}\".format(loss))\n",
    "        \n",
    "print(\"Done!\")\n",
    "game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model.save(\"dqn-predict-v1.h5\")\n",
    "training_model.save(\"dqn-train-v1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "game.close()\n",
    "game.set_episode_timeout(num_steps)\n",
    "game.init()\n",
    "\n",
    "for episode in range(100):\n",
    "    game.new_episode()\n",
    "    \n",
    "    # Init frame stack\n",
    "    frame_stack = deque(maxlen=4)\n",
    "    initial_frame = preprocess_frame(game.get_state().screen_buffer)\n",
    "    for _ in range(4):\n",
    "        frame_stack.append(initial_frame)\n",
    "    state = np.stack(frame_stack, axis=2)\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Get action with highest Q-value for current state\n",
    "        action = np.argmax(prediction_model.predict_on_batch(np.expand_dims(state, axis=0)))\n",
    "        action_one_hot = [False] * 3\n",
    "        action_one_hot[action] = True\n",
    "\n",
    "        # Take action and get reward\n",
    "        reward = game.make_action(action_one_hot)\n",
    "        done = game.is_episode_finished()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        frame_stack.append(preprocess_frame(game.get_state().screen_buffer))\n",
    "        new_state = np.stack(frame_stack, axis=2)\n",
    "        state = new_state\n",
    "        time.sleep(0.016)\n",
    "print(\"Done!\")\n",
    "game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
