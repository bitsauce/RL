{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import vizdoom\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from skimage import transform\n",
    "from IPython.display import display, clear_output\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes       = 500\n",
    "learning_rate      = 0.0002\n",
    "discount_factor    = 0.99\n",
    "num_envs           = 16\n",
    "num_actions        = 5\n",
    "\n",
    "t_max            = 5\n",
    "I_Update         = 5\n",
    "frame_stack_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C():\n",
    "    def __init__(self, num_actions, optimizer=tf.train.RMSPropOptimizer(0.0001)):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Construct model\n",
    "        self.input_states = tf.placeholder(shape=(None, 84, 84, 4), dtype=tf.float32)\n",
    "        conv1 = tf.keras.layers.Conv2D(32, (3, 3), activation=\"elu\", padding=\"valid\", input_shape=(84, 84, 4))(self.input_states)\n",
    "        pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        conv2 = tf.keras.layers.Conv2D(64, (3, 3), activation=\"elu\", padding=\"valid\")(pool1)\n",
    "        pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        conv3 = tf.keras.layers.Conv2D(128, (3, 3), activation=\"elu\", padding=\"valid\")(pool2)\n",
    "        shared_features = tf.keras.layers.Flatten()(conv3)\n",
    "        \n",
    "        # Policy branch\n",
    "        dense1            = tf.keras.layers.Dense(16, activation=\"relu\")(shared_features)\n",
    "        self.action_prob  = tf.keras.layers.Dense(num_actions, activation=\"softmax\")(dense1)\n",
    "        \n",
    "        # Baseline value branch\n",
    "        dense2              = tf.keras.layers.Dense(512, activation=\"elu\")(shared_features)\n",
    "        dense3              = tf.keras.layers.Dense(128, activation=\"elu\")(dense2)\n",
    "        self.baseline_value = tf.keras.layers.Dense(1, activation=None)(dense3) # V(s_t; θ_v)\n",
    "        \n",
    "        # Create policy gradient train function\n",
    "        self.actions_onehot_placeholder = tf.placeholder(shape=(None, num_actions), dtype=tf.float32)\n",
    "        self.returns_placeholder        = tf.placeholder(shape=(None,), dtype=tf.float32) # R_t\n",
    "        \n",
    "        # Get probabilities of taken actions: log π(a_t | s_t; θ)\n",
    "        log_action_prob = tf.log(tf.reduce_sum(self.action_prob * self.actions_onehot_placeholder, axis=1))\n",
    "        \n",
    "        # Loss = ∇_θ log π(a_t | s_t; θ)(R_t − V(s_t; θ_v))\n",
    "        # Negative log likelihood of the taken actions,\n",
    "        # weighted by the discounted and normalized rewards\n",
    "        self.loss = -tf.reduce_mean(log_action_prob * (self.returns_placeholder - self.baseline_value)) # + TODO: entropy\n",
    "        \n",
    "        # Create gradient accumulator\n",
    "        tvs = tf.trainable_variables()\n",
    "        accum_vars = [tf.Variable(tf.zeros_like(tv.initialized_value()), trainable=False) for tv in tvs]\n",
    "        gvs = optimizer.compute_gradients(self.loss, tvs)\n",
    "        \n",
    "        self.accum_grads = [accum_vars[i].assign_add(gv[0]) for i, gv in enumerate(gvs)]\n",
    "        self.reset_grads = [tv.assign(tf.zeros_like(tv)) for tv in accum_vars]\n",
    "        self.train_step = optimizer.apply_gradients([(accum_vars[i], gv[1]) for i, gv in enumerate(gvs)])\n",
    "        \n",
    "        # Create session\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "        # Run the initializer\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def apply_gradients(self):\n",
    "        return self.sess.run([self.train_step])\n",
    "        \n",
    "    def reset_gradients(self):\n",
    "        return self.sess.run([self.reset_grads], feed_dict={})\n",
    "    \n",
    "    def accumulate_gradients(self, input_states, actions_onehot, returns):\n",
    "        return self.sess.run([self.accum_grads, self.loss],\n",
    "                             feed_dict={self.input_states: input_states,\n",
    "                                        self.actions_onehot_placeholder: actions_onehot,\n",
    "                                        self.returns_placeholder: returns})[1]\n",
    "        \n",
    "    def predict_action(self, input_states):\n",
    "        return self.sess.run(self.action_prob, feed_dict={self.input_states: input_states})\n",
    "    \n",
    "    def predict_value(self, input_states):\n",
    "        return self.sess.run(self.baseline_value, feed_dict={self.input_states: input_states})\n",
    "    \n",
    "    \n",
    "a2c_model = A2C(num_actions=num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoomEnv():\n",
    "    def __init__(self, show_window=False):\n",
    "        # Setup DoomGame\n",
    "        self.game = vizdoom.DoomGame()\n",
    "        self.game.load_config(\"doom/my_way_home.cfg\")\n",
    "\n",
    "        # Visualize the game (set to False to train faster)\n",
    "        self.game.set_window_visible(show_window)\n",
    "\n",
    "        # Set screen format to greyscale, improves training time\n",
    "        self.game.set_screen_format(vizdoom.ScreenFormat.GRAY8)\n",
    "\n",
    "        # Make the game end after 2100 ticks (set to 0 to disable)\n",
    "        self.game.set_episode_timeout(2100)\n",
    "\n",
    "        # Init game\n",
    "        self.game.init()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.game.new_episode()\n",
    "        \n",
    "        # Setup initial state\n",
    "        self.frame_stack = deque(maxlen=4)\n",
    "        initial_frame = preprocess_frame(self.game.get_state().screen_buffer)\n",
    "        for _ in range(4):\n",
    "            self.frame_stack.append(initial_frame)\n",
    "        self.state = np.stack(self.frame_stack, axis=2)\n",
    "        \n",
    "envs = [DoomEnv(i == 0) for i in range(num_envs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Episode 4/500 --\n",
      "Episode loss 0.0\n"
     ]
    }
   ],
   "source": [
    "def preprocess_frame(frame):\n",
    "    cropped_frame = frame[30:-10, 30:-30] # Crop the screen\n",
    "    normalized_frame = cropped_frame / 255.0 # Normalize Pixel Values\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [84, 84]) # Resize\n",
    "    return preprocessed_frame\n",
    "\n",
    "episode_loss = float(\"nan\")\n",
    "for episode in range(num_episodes):\n",
    "    clear_output(wait=True)\n",
    "    print(\"-- Episode {}/{} --\".format(episode, num_episodes))\n",
    "    print(\"Episode loss\", episode_loss)\n",
    "    for env in envs:\n",
    "        env.reset()\n",
    "\n",
    "    episode_loss = 0\n",
    "    while True:\n",
    "        for env in envs:\n",
    "            if env.game.is_episode_finished():\n",
    "                break\n",
    "                    \n",
    "            a2c_model.reset_gradients()\n",
    "            rewards = []\n",
    "            states  = []\n",
    "            actions = []\n",
    "            for _ in range(t_max):\n",
    "                # Predict action given state: π(a_t | s_t; θ)\n",
    "                action_prob = np.squeeze(a2c_model.predict_action(np.expand_dims(env.state, axis=0)))\n",
    "                action = np.random.choice(np.arange(0, num_actions), p=action_prob) # Sample action stochastically\n",
    "                action_one_hot = [False] * num_actions\n",
    "                action_one_hot[action] = True\n",
    "\n",
    "                states.append(env.state)\n",
    "                actions.append(action_one_hot)\n",
    "\n",
    "                #print(\"Taking action \",action)\n",
    "\n",
    "                # Take the action\n",
    "                rewards.append(env.game.make_action(action_one_hot))\n",
    "\n",
    "                if env.game.is_episode_finished():\n",
    "                    break\n",
    "\n",
    "                # Get new state\n",
    "                env.frame_stack.append(preprocess_frame(env.game.get_state().screen_buffer))\n",
    "                env.state = np.stack(env.frame_stack, axis=2)\n",
    "\n",
    "\n",
    "            R = 0 if env.game.is_episode_finished() else a2c_model.predict_value(np.expand_dims(env.state, axis=0))[0][0]\n",
    "            discounted_rewards = [R]\n",
    "            #print(\"Accumulating gradients\")\n",
    "            for i in range(len(rewards)-2, -1, -1):\n",
    "                R = rewards[i] + discount_factor * R\n",
    "                discounted_rewards.append(R)\n",
    "\n",
    "            #print(np.array(states).shape, np.array(actions).shape, np.array(discounted_rewards).shape)\n",
    "            episode_loss += a2c_model.accumulate_gradients(states, actions, discounted_rewards)\n",
    "\n",
    "            #print(\"Train step\")\n",
    "            a2c_model.apply_gradients()\n",
    "\n",
    "        # If all environments are done, break\n",
    "        if sum([env.game.is_episode_finished() for env in envs]) == num_envs:\n",
    "            break\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy = True\n",
    "for episode in range(1):\n",
    "    env = envs[0]\n",
    "    env.reset()\n",
    "    while not env.game.is_episode_finished():\n",
    "        # Predict action given state: π(a_t | s_t; θ)\n",
    "        action_prob = np.squeeze(a2c_model.predict_action(np.expand_dims(env.state, axis=0)))\n",
    "        if greedy:\n",
    "            action = np.argmax(action_prob)\n",
    "        else:\n",
    "            action = np.random.choice(np.arange(0, num_actions), p=action_prob) # Sample action stochastically\n",
    "        action_one_hot = [False] * num_actions\n",
    "        action_one_hot[action] = True\n",
    "\n",
    "        # Take the action\n",
    "        env.game.make_action(action_one_hot)\n",
    "        \n",
    "        if not env.game.is_episode_finished():\n",
    "            # Get new state\n",
    "            env.frame_stack.append(preprocess_frame(env.game.get_state().screen_buffer))\n",
    "            env.state = np.stack(env.frame_stack, axis=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
