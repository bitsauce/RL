{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import vizdoom\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import keras\n",
    "import keras.models\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras import backend as K\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from skimage import transform\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes       = 500\n",
    "num_steps          = 100\n",
    "replay_buffer_size = 1000000\n",
    "learning_rate      = 0.0002\n",
    "discount_factor    = 0.95\n",
    "batch_size         = 4#64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    cropped_frame = frame[30:-10, 30:-30] # Crop the screen\n",
    "    normalized_frame = cropped_frame / 255.0 # Normalize Pixel Values\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [84, 84]) # Resize\n",
    "    return preprocessed_frame\n",
    "        \n",
    "class DoomInstance():\n",
    "    def __init__(self):\n",
    "        # Setup DoomGame\n",
    "        self.game = vizdoom.DoomGame()\n",
    "        self.game.load_config(\"doom/my_way_home.cfg\")\n",
    "\n",
    "        # Visualize the game (set to False to train faster)\n",
    "        self.game.set_window_visible(False)\n",
    "\n",
    "        # Set screen format to greyscale, improves training time\n",
    "        self.game.set_screen_format(vizdoom.ScreenFormat.GRAY8)\n",
    "\n",
    "        # Make the game end after 2100 ticks (set to 0 to disable)\n",
    "        self.game.set_episode_timeout(2100)\n",
    "\n",
    "        # Init game\n",
    "        self.game.init()\n",
    "        self.num_actions = self.game.get_available_buttons_size()\n",
    "            \n",
    "    def run_episode(self, a2c_model):\n",
    "        self.game.new_episode()\n",
    "        \n",
    "        # Init frame stack\n",
    "        frame_stack = deque(maxlen=4)\n",
    "        initial_frame = preprocess_frame(self.game.get_state().screen_buffer)\n",
    "        for _ in range(4):\n",
    "            frame_stack.append(initial_frame)\n",
    "        state = np.stack(frame_stack, axis=2)\n",
    "        \n",
    "        batch = []\n",
    "        while not self.game.is_episode_finished():\n",
    "            \n",
    "            # Predict action given state pi(a_t|s_t)\n",
    "            action_prob = np.squeeze(a2c_model.predict_policy([np.expand_dims(state, axis=0)]))\n",
    "            action = np.random.choice(np.arange(0, self.num_actions), p=action_prob)\n",
    "            action_one_hot = [False] * self.num_actions\n",
    "            action_one_hot[action] = True\n",
    "            \n",
    "            # Take the action\n",
    "            reward = self.game.make_action(action_one_hot)\n",
    "            \n",
    "            # If not done\n",
    "            if not self.game.is_episode_finished():\n",
    "                # Store the experience\n",
    "                frame_stack.append(preprocess_frame(self.game.get_state().screen_buffer))\n",
    "                new_state = np.stack(frame_stack, axis=2)\n",
    "                batch.append((state, action_one_hot, reward, new_state))\n",
    "                #batch.append((a2c_model.predict_value([np.expand_dims(state, axis=0)]), action_one_hot, reward))\n",
    "                \n",
    "        return batch\n",
    "    \n",
    "def run_episodes(game_instances, policy_model, value_model):\n",
    "    pool = ThreadPool()\n",
    "    results = []\n",
    "    for instance in game_instances:\n",
    "        results.append(pool.apply_async(instance.run_episode, args=(policy_model, value_model)))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return [r.get() for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C():\n",
    "    def __init__(self, num_actions, optimizer=Adam()):\n",
    "        # Construct model\n",
    "        input_states = Input(shape=(84, 84, 4), dtype=\"float32\")\n",
    "        conv1 = Conv2D(32, (3, 3), activation=\"elu\", padding=\"valid\", input_shape=(84, 84, 4))(input_states)\n",
    "        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        conv2 = Conv2D(64, (3, 3), activation=\"elu\", padding=\"valid\")(pool1)\n",
    "        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        conv3 = Conv2D(128, (3, 3), activation=\"elu\", padding=\"valid\")(pool2)\n",
    "        shared_features = Flatten()(conv3)\n",
    "        \n",
    "        # Policy branch\n",
    "        dense1       = Dense(16, activation=\"relu\")(shared_features)\n",
    "        action_prob  = Dense(num_actions, activation=\"softmax\")(dense1)\n",
    "        \n",
    "        # Value branch\n",
    "        dense2 = Dense(512, activation=\"elu\")(shared_features)\n",
    "        dense3 = Dense(128, activation=\"elu\")(dense2)\n",
    "        value  = Dense(1, activation=None)(dense3)\n",
    "        \n",
    "        # Policy model\n",
    "        self.policy_model = keras.models.Model(input_states, action_prob)\n",
    "                \n",
    "        # Value model\n",
    "        self.value_model = keras.models.Model(input_states, value)\n",
    "        \n",
    "        # Create prediction function\n",
    "        self.predict_policy_fn = K.function(inputs=[self.policy_model.input],\n",
    "                                            outputs=[self.policy_model.output],\n",
    "                                            updates=[])\n",
    "        \n",
    "        # Create policy gradient train function\n",
    "        action_onehot_placeholder   = K.placeholder(shape=(None, num_actions))\n",
    "        discount_reward_placeholder = K.placeholder(shape=(None,))\n",
    "        \n",
    "        # Get probabilities of taken actions\n",
    "        log_action_prob = K.log(K.sum(self.policy_model.output * action_onehot_placeholder, axis=1))\n",
    "        \n",
    "        # loss\n",
    "        # Negative log likelihood of the taken actions,\n",
    "        # weighted by the discounted and normalized rewards\n",
    "        loss = K.mean(discount_reward_placeholder * -log_action_prob)\n",
    "        updates = optimizer.get_updates(params=self.policy_model.trainable_weights, loss=loss)\n",
    "        self.train_policy_fn = K.function(inputs=[input_states, action_onehot_placeholder, discount_reward_placeholder],\n",
    "                                          outputs=[],\n",
    "                                          updates=updates)\n",
    "        \n",
    "        self.predict_value_fn = K.function(inputs=[self.value_model.input],\n",
    "                                           outputs=[self.value_model.output],\n",
    "                                           updates=[])\n",
    "        \n",
    "        loss = K.mean(value)\n",
    "        updates = optimizer.get_updates(params=self.value_model.trainable_weights, loss=loss)\n",
    "        self.train_value_fn = K.function(inputs=[self.value_model.input, action_onehot_placeholder, discount_reward_placeholder],\n",
    "                                         outputs=[],\n",
    "                                         updates=updates)\n",
    "    \n",
    "    def train_policy(self, inputs):\n",
    "        self.train_policy_fn(inputs)\n",
    "        \n",
    "    def predict_policy(self, inputs):\n",
    "        return self.predict_policy_fn(inputs)\n",
    "    \n",
    "    def train_value(self, inputs):\n",
    "        self.train_value_fn(inputs)\n",
    "        \n",
    "    def predict_value(self, inputs):\n",
    "        return self.predict_value_fn(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "doom_instance = DoomInstance()\n",
    "a2c_model = A2C(num_actions=doom_instance.game.get_available_buttons_size())\n",
    "trajectory = doom_instance.run_episode(a2c_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_batch      = trajectory#random.sample(trajectory, batch_size)\n",
    "replay_state      = [r[0] for r in replay_batch]\n",
    "replay_action     = [r[1] for r in replay_batch]\n",
    "replay_reward     = [r[2] for r in replay_batch]\n",
    "replay_next_state = [r[3] for r in replay_batch]\n",
    "\n",
    "last_reward = a2c_model.predict_value([np.expand_dims(replay_state[-1], axis=0)])\n",
    "\n",
    "for i in range(len(replay_batch)):\n",
    "    reward = replay_reward[i] + discount_fator * last_reward\n",
    "    \n",
    "    discounted_rewards = reward - a2c_model.predict_value([np.expand_dims(replay_state[i], axis=0)])\n",
    "    a2c_model.train_policy([np.stack(states, axis=0), np.stack(actions, axis=0), discounted_rewards])\n",
    "    a2c_model.train_value([replay_state, replay_action, Q_target])\n",
    "    \n",
    "    # Calculate discounted rewards\n",
    "    #discounted_rewards = discount_rewards(episode_rewards)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
