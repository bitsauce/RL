{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import vizdoom\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from skimage import transform\n",
    "from IPython.display import display, clear_output\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduler():\n",
    "    def __init__(self, initial_value, interval, decay_factor):\n",
    "        self.interval = self.counter = interval\n",
    "        self.decay_factor = decay_factor\n",
    "        self.value_factor = 1\n",
    "        self.value = initial_value\n",
    "        \n",
    "    def get_value(self):\n",
    "        self.counter -= 1\n",
    "        if self.counter < 0:\n",
    "            self.counter = self.interval\n",
    "            self.value *= self.decay_factor\n",
    "        return self.value\n",
    "        \n",
    "lr_scheduler = Scheduler(initial_value=1e-4, interval=10, decay_factor=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoomEnv():\n",
    "    def __init__(self, show_window=False):\n",
    "        # Setup DoomGame\n",
    "        self.game = vizdoom.DoomGame()\n",
    "        #self.game.load_config(\"doom/my_way_home.cfg\")\n",
    "        #self.game.load_config(\"doom/defend_the_center.cfg\")\n",
    "        self.game.load_config(\"doom/basic.cfg\")\n",
    "\n",
    "        # Visualize the game (set to False to train faster)\n",
    "        self.game.set_window_visible(show_window)\n",
    "\n",
    "        # Set screen format to greyscale, improves training time\n",
    "        self.game.set_screen_format(vizdoom.ScreenFormat.GRAY8)\n",
    "\n",
    "        # Make the game end after 2100 ticks (set to 0 to disable)\n",
    "        #self.game.set_episode_timeout(2100)\n",
    "\n",
    "        # Init game\n",
    "        self.game.init()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.game.new_episode()\n",
    "        \n",
    "        # Setup initial state\n",
    "        self.frame_stack = deque(maxlen=frame_stack_size)\n",
    "        initial_frame = preprocess_frame(self.game.get_state().screen_buffer)\n",
    "        for _ in range(frame_stack_size):\n",
    "            self.frame_stack.append(initial_frame)\n",
    "        self.state = np.stack(self.frame_stack, axis=2)\n",
    "        \n",
    "        self.total_reward = 0\n",
    "        \n",
    "num_envs = 16\n",
    "envs = [DoomEnv(i == 0) for i in range(num_envs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_factor  = 0.99\n",
    "num_actions      = envs[0].game.get_available_buttons_size()\n",
    "save_interval    = 100\n",
    "t_max            = 5\n",
    "frame_stack_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(logits):\n",
    "    a0 = logits - tf.reduce_max(logits, axis=-1, keepdims=True)\n",
    "    ea0 = tf.exp(a0)\n",
    "    z0 = tf.reduce_sum(ea0, axis=-1, keepdims=True)\n",
    "    p0 = ea0 / z0\n",
    "    return tf.reduce_sum(p0 * (tf.log(z0) - a0), axis=-1)\n",
    "\n",
    "class A2C():\n",
    "    def __init__(self, num_actions, optimizer, value_scale=0.5, entropy_scale=0.01, model_checkpoint=None):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Construct model\n",
    "        self.input_states = tf.placeholder(shape=(None, 84, 84, 4), dtype=tf.float32)\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"valid\")(self.input_states)\n",
    "        self.pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(self.conv1)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"valid\")(self.pool1)\n",
    "        self.pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(self.conv2)\n",
    "        self.conv3 = tf.keras.layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"valid\")(self.pool2)\n",
    "        self.shared_features = tf.keras.layers.Flatten()(self.conv3)\n",
    "        \n",
    "        # Policy branch\n",
    "        self.dense1        = tf.keras.layers.Dense(512, activation=\"relu\")(self.shared_features)\n",
    "        self.dense2        = tf.keras.layers.Dense(128, activation=\"relu\")(self.dense1)\n",
    "        self.action_logits = tf.keras.layers.Dense(num_actions, activation=None)(self.dense2)\n",
    "        self.action_prob   = tf.keras.layers.Softmax()(self.action_logits)\n",
    "        \n",
    "        # Baseline value branch\n",
    "        self.dense3 = tf.keras.layers.Dense(512, activation=\"relu\")(self.shared_features)\n",
    "        self.dense4 = tf.keras.layers.Dense(128, activation=\"relu\")(self.dense3)\n",
    "        self.value  = tf.keras.layers.Dense(1, activation=None)(self.dense4) # V(s_t; θ_v)\n",
    "        \n",
    "        # Create policy gradient train function\n",
    "        self.actions_placeholder = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "        self.returns_placeholder = tf.placeholder(shape=(None,), dtype=tf.float32)\n",
    "        self.values_placeholder  = tf.placeholder(shape=(None,), dtype=tf.float32)\n",
    "        self.lr_placeholder      = tf.placeholder(shape=(), dtype=tf.float32)\n",
    "        \n",
    "        # Get probabilities of taken actions: log π(a_t | s_t; θ)\n",
    "        # log_action_prob = tf.log(tf.reduce_sum(self.action_prob * self.actions_onehot_placeholder, axis=1))\n",
    "        self.neg_log_action = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.action_logits,\n",
    "                                                                             labels=self.actions_placeholder)\n",
    "        \n",
    "        # Policy Gradient Loss = ∇_θ log π(a_t | s_t; θ)(R_t − V(s_t; θ_v))\n",
    "        # Negative log likelihood of the taken actions, weighted by the discounted and normalized rewards\n",
    "        # self.policy_loss = -tf.reduce_mean(log_action_prob * (self.returns_placeholder - self.value))\n",
    "        self.policy_loss  = tf.reduce_mean((self.returns_placeholder - self.values_placeholder) * self.neg_log_action)\n",
    "        \n",
    "        # Get value loss\n",
    "        # MSE(V(s_t), R_t)\n",
    "        self.value_loss = tf.reduce_mean(tf.squared_difference(tf.squeeze(self.value), self.returns_placeholder))\n",
    "        \n",
    "        # Get entropy\n",
    "        # self.entropy_loss = -tf.reduce_mean(self.action_prob * tf.log(self.action_prob + 0.0001))\n",
    "        self.entropy_loss = tf.reduce_mean(entropy(self.action_logits))\n",
    "        \n",
    "        # Total loss\n",
    "        self.loss = self.policy_loss + self.value_loss * value_scale - self.entropy_loss * entropy_scale\n",
    "        \n",
    "        # Minimize loss\n",
    "        self.optimizer = optimizer(learning_rate=self.lr_placeholder, decay=0.99)\n",
    "        self.learning_rate = 1e-4\n",
    "        self.train_step = self.optimizer.minimize(self.loss)\n",
    "        \n",
    "        # Create session\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "        # Run the initializer\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        tf.summary.scalar(\"policy_loss\", self.policy_loss)\n",
    "        tf.summary.scalar(\"value_loss\", self.value_loss)\n",
    "        tf.summary.scalar(\"entropy_loss\", self.entropy_loss)\n",
    "        tf.summary.scalar(\"loss\", self.loss)\n",
    "        tf.summary.scalar(\"learning_rate\", self.lr_placeholder)\n",
    "        self.summary_merged = tf.summary.merge_all()\n",
    "        \n",
    "        # Load model checkpoint if provided\n",
    "        self.saver = tf.train.Saver()\n",
    "        if model_checkpoint:\n",
    "            self.run_idx = int(re.findall(r\"_run\\d+\", model_checkpoint)[0][len(\"_run\"):])\n",
    "            self.step_idx = int(re.findall(r\"_step\\d+\", model_checkpoint)[0][len(\"_step\"):])\n",
    "            self.saver.restore(self.sess, model_checkpoint)\n",
    "            print(\"Model checkpoint restored from {}\".format(model_checkpoint))\n",
    "        else:\n",
    "            self.run_idx = 0\n",
    "            while os.path.isdir(\"./logs/run{}\".format(self.run_idx)):\n",
    "                self.run_idx += 1\n",
    "            self.step_idx = 0\n",
    "        self.train_writer = tf.summary.FileWriter(\"./logs/run{}\".format(self.run_idx), self.sess.graph)\n",
    "        \n",
    "        \n",
    "    def save(self):\n",
    "        model_checkpoint = \"./models/a2c_run{}_step{}.ckpt\".format(self.run_idx, self.step_idx)\n",
    "        self.saver.save(self.sess, model_checkpoint)\n",
    "        print(\"Model checkpoint saved to {}\".format(model_checkpoint))\n",
    "        \n",
    "    def train(self, input_states, actions, returns, values):\n",
    "        r = self.sess.run([self.summary_merged, self.train_step, self.loss, self.policy_loss, self.value_loss, self.entropy_loss],\n",
    "                          feed_dict={self.input_states: input_states,\n",
    "                                     self.actions_placeholder: actions,\n",
    "                                     self.returns_placeholder: returns,\n",
    "                                     self.values_placeholder: values,\n",
    "                                     self.lr_placeholder: self.learning_rate})\n",
    "        self.train_writer.add_summary(r[0], self.step_idx)\n",
    "        self.step_idx += 1\n",
    "        return r[2:]\n",
    "        \n",
    "    def predict(self, input_states):\n",
    "        return self.sess.run([self.action_prob, self.value], feed_dict={self.input_states: input_states})\n",
    "    \n",
    "a2c_model = A2C(num_actions=num_actions, optimizer=tf.train.RMSPropOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting envronments...\n",
      "Training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-73d113604b8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;31m# Get new state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_stack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-73d113604b8b>\u001b[0m in \u001b[0;36mpreprocess_frame\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcropped_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Crop the screen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mnormalized_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcropped_frame\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m \u001b[0;31m# Normalize Pixel Values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpreprocessed_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m84\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m84\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Resize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpreprocessed_frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def preprocess_frame(frame):\n",
    "    cropped_frame = frame[30:-10, 30:-30] # Crop the screen\n",
    "    normalized_frame = cropped_frame / 255.0 # Normalize Pixel Values\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [84, 84]) # Resize\n",
    "    return preprocessed_frame\n",
    "\n",
    "def calculate_expected_return(rewards, gamma):\n",
    "    expected_return = []\n",
    "    r = 0\n",
    "    for reward in rewards[::-1]: # for rewards from end to start\n",
    "        r = reward + gamma * r\n",
    "        expected_return.append(r)\n",
    "    return expected_return[::-1] # reverse so that we get the expected return from start to end\n",
    "\n",
    "average_episode_rewards = []\n",
    "#for episode in range(num_episodes):\n",
    "episode = 0\n",
    "while True:\n",
    "    print(\"Resetting envronments...\")\n",
    "    episode += 1\n",
    "    for env in envs:\n",
    "        env.reset()\n",
    "    \n",
    "    # While there are running environments\n",
    "    print(\"Training...\")\n",
    "    a2c_model.learning_rate = lr_scheduler.get_value()\n",
    "    episode_loss = episode_policy_loss = episode_value_loss = episode_entropy_loss = 0\n",
    "    average_episode_reward = []\n",
    "    while sum([env.game.is_episode_finished() for env in envs]) < num_envs:\n",
    "        states, actions, returns, values = [], [], [], []\n",
    "        \n",
    "        # For every environment\n",
    "        for env in envs:\n",
    "            # Simulate game for some number of steps\n",
    "            rewards = []\n",
    "            for _ in range(t_max):\n",
    "                # Predict and value action given state\n",
    "                # π(a_t | s_t; θ)\n",
    "                action_prob, value = a2c_model.predict(np.expand_dims(env.state, axis=0))\n",
    "                action_prob, value = np.squeeze(action_prob), np.squeeze(value)\n",
    "                \n",
    "                # Take action stochastically \n",
    "                action = np.random.choice(np.arange(0, num_actions), p=action_prob)\n",
    "                action_one_hot = [False] * num_actions\n",
    "                action_one_hot[action] = True\n",
    "                reward = env.game.make_action(action_one_hot) * 0.001\n",
    "                \n",
    "                # Store state, action and reward\n",
    "                states.append(env.state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                values.append(value)\n",
    "                env.total_reward += reward\n",
    "\n",
    "                if env.game.is_episode_finished():\n",
    "                    break\n",
    "                    \n",
    "                # Get new state\n",
    "                env.frame_stack.append(preprocess_frame(env.game.get_state().screen_buffer))\n",
    "                env.state = np.stack(env.frame_stack, axis=2)\n",
    "                \n",
    "            # Calculate return (discounted rewards over a trajectory)\n",
    "            last_value = 0 if env.game.is_episode_finished() else \\\n",
    "                         a2c_model.predict(np.expand_dims(env.state, axis=0))[1][0][0]\n",
    "            returns.extend(calculate_expected_return(rewards+[last_value], discount_factor)[:-1])\n",
    "            average_episode_reward.extend(rewards)\n",
    "            \n",
    "        eploss, pgloss, vloss, entloss = a2c_model.train(states, actions, returns, values)\n",
    "        episode_loss         += eploss\n",
    "        episode_policy_loss  += pgloss\n",
    "        episode_value_loss   += vloss\n",
    "        episode_entropy_loss += entloss\n",
    "    average_episode_rewards.append(sum([env.total_reward for env in envs]) / len(envs))\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(\"-- Episode {} --\".format(episode))\n",
    "    print(\"Learning rate:\", a2c_model.learning_rate)\n",
    "    print(\"Episode policy loss:\", episode_policy_loss)\n",
    "    print(\"Episode value loss:\", episode_value_loss)\n",
    "    print(\"Episode entropy loss:\", episode_entropy_loss)\n",
    "    print(\"Episode loss:\", episode_loss)\n",
    "    print(\"Average episode reward:\", average_episode_rewards[-1])\n",
    "    print(\"\")\n",
    "    plt.plot(np.arange(0, len(average_episode_rewards)), average_episode_rewards)\n",
    "    plt.show()\n",
    "    \n",
    "    if episode % save_interval == 0:\n",
    "        a2c_model.save()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-cc94ec76b3af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Take the action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_one_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.016\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_episode_finished\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "env = DoomEnv(True)\n",
    "greedy = True\n",
    "for episode in range(10):\n",
    "    env.reset()\n",
    "    while not env.game.is_episode_finished():\n",
    "        # Predict action given state: π(a_t | s_t; θ)\n",
    "        action_prob = np.squeeze(a2c_model.predict(np.expand_dims(env.state, axis=0))[0])\n",
    "        if greedy:\n",
    "            action = np.argmax(action_prob)\n",
    "        else:\n",
    "            action = np.random.choice(np.arange(0, num_actions), p=action_prob) # Sample action stochastically\n",
    "        action_one_hot = [False] * num_actions\n",
    "        action_one_hot[action] = True\n",
    "\n",
    "        # Take the action\n",
    "        env.game.make_action(action_one_hot)\n",
    "        time.sleep(0.016)\n",
    "        \n",
    "        if not env.game.is_episode_finished():\n",
    "            # Get new state\n",
    "            env.frame_stack.append(preprocess_frame(env.game.get_state().screen_buffer))\n",
    "            env.state = np.stack(env.frame_stack, axis=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
