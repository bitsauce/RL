{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import vizdoom\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from skimage import transform\n",
    "from IPython.display import display, clear_output\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduler():\n",
    "    def __init__(self, initial_value, interval, decay_factor):\n",
    "        self.interval = self.counter = interval\n",
    "        self.decay_factor = decay_factor\n",
    "        self.value_factor = 1\n",
    "        self.value = initial_value\n",
    "        \n",
    "    def get_value(self):\n",
    "        self.counter -= 1\n",
    "        if self.counter < 0:\n",
    "            self.counter = self.interval\n",
    "            self.value *= self.decay_factor\n",
    "        return self.value\n",
    "        \n",
    "lr_scheduler = Scheduler(initial_value=1e-2, interval=20, decay_factor=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes     = 500\n",
    "discount_factor  = 0.99\n",
    "num_envs         = 16\n",
    "num_actions      = 5\n",
    "t_max            = 5\n",
    "frame_stack_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(logits):\n",
    "    a0 = logits - tf.reduce_max(logits, axis=-1, keepdims=True)\n",
    "    ea0 = tf.exp(a0)\n",
    "    z0 = tf.reduce_sum(ea0, axis=-1, keepdims=True)\n",
    "    p0 = ea0 / z0\n",
    "    return tf.reduce_sum(p0 * (tf.log(z0) - a0), axis=-1)\n",
    "\n",
    "class A2C():\n",
    "    def __init__(self, num_actions, optimizer, value_scale=0.5, entropy_scale=0.001):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Construct model\n",
    "        self.input_states = tf.placeholder(shape=(None, 84, 84, 4), dtype=tf.float32)\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, (3, 3), activation=\"elu\", padding=\"valid\")(self.input_states)\n",
    "        self.pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(self.conv1)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, (3, 3), activation=\"elu\", padding=\"valid\")(self.pool1)\n",
    "        self.pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(self.conv2)\n",
    "        self.conv3 = tf.keras.layers.Conv2D(128, (3, 3), activation=\"elu\", padding=\"valid\")(self.pool2)\n",
    "        self.shared_features = tf.keras.layers.Flatten()(self.conv3)\n",
    "        \n",
    "        # Policy branch\n",
    "        self.dense1       = tf.keras.layers.Dense(16, activation=\"relu\")(self.shared_features)\n",
    "        self.action_prob  = tf.keras.layers.Dense(num_actions, activation=\"softmax\")(self.dense1)\n",
    "        \n",
    "        # Baseline value branch\n",
    "        self.dense2 = tf.keras.layers.Dense(512, activation=\"elu\")(self.shared_features)\n",
    "        self.dense3 = tf.keras.layers.Dense(128, activation=\"elu\")(self.dense2)\n",
    "        self.value  = tf.keras.layers.Dense(1, activation=None)(self.dense3) # V(s_t; θ_v)\n",
    "        \n",
    "        # Create policy gradient train function\n",
    "        self.actions_placeholder = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "        self.returns_placeholder = tf.placeholder(shape=(None,), dtype=tf.float32)\n",
    "        self.lr_placeholder      = tf.placeholder(shape=(), dtype=tf.float32)\n",
    "        \n",
    "        # Get probabilities of taken actions: log π(a_t | s_t; θ)\n",
    "        # log_action_prob = tf.log(tf.reduce_sum(self.action_prob * self.actions_onehot_placeholder, axis=1))\n",
    "        self.log_action_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.action_prob,\n",
    "                                                                              labels=self.actions_placeholder)\n",
    "        \n",
    "        # Policy Gradient Loss = ∇_θ log π(a_t | s_t; θ)(R_t − V(s_t; θ_v))\n",
    "        # Negative log likelihood of the taken actions, weighted by the discounted and normalized rewards\n",
    "        self.policy_loss  = tf.reduce_mean((self.returns_placeholder - self.value) * self.log_action_prob)\n",
    "        #self.policy_loss = -tf.reduce_mean(log_action_prob * (self.returns_placeholder - self.value))\n",
    "        \n",
    "        # Get value loss\n",
    "        # MSE(V(s_t), R_t)\n",
    "        self.value_loss = tf.reduce_mean(tf.squared_difference(tf.squeeze(self.value), self.returns_placeholder))\n",
    "        \n",
    "        # Get entropy\n",
    "        self.entropy_loss = tf.reduce_mean(entropy(self.action_prob))\n",
    "        #self.entropy_loss = -tf.reduce_mean(self.action_prob * tf.log(self.action_prob + 0.0001))\n",
    "        \n",
    "        # Total loss\n",
    "        self.loss = self.policy_loss + self.value_loss * value_scale - self.entropy_loss * entropy_scale\n",
    "        \n",
    "        # Minimize loss\n",
    "        self.optimizer = optimizer(learning_rate=self.lr_placeholder)\n",
    "        self.learning_rate = 1e-4\n",
    "        self.train_step = self.optimizer.minimize(self.loss)\n",
    "        \n",
    "        # Create session\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "        # Run the initializer\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        tf.summary.scalar(\"policy_loss\", self.policy_loss)\n",
    "        tf.summary.scalar(\"value_loss\", self.value_loss)\n",
    "        tf.summary.scalar(\"entropy_loss\", self.entropy_loss)\n",
    "        tf.summary.scalar(\"loss\", self.loss)\n",
    "        self.summary_merged = tf.summary.merge_all()\n",
    "        \n",
    "        run_idx = 0\n",
    "        while os.path.isdir(\"./logs/run{}\".format(run_idx)):\n",
    "            run_idx += 1\n",
    "        self.train_writer = tf.summary.FileWriter(\"./logs/run{}\".format(run_idx), self.sess.graph)\n",
    "        self.step_idx = 0\n",
    "        \n",
    "    def train(self, input_states, actions, returns, values):\n",
    "        r = self.sess.run([self.summary_merged, self.train_step, self.loss, self.policy_loss, self.value_loss, self.entropy_loss],\n",
    "                          feed_dict={self.input_states: input_states,\n",
    "                                     self.actions_placeholder: actions,\n",
    "                                     self.returns_placeholder: returns,\n",
    "                                     self.lr_placeholder: self.learning_rate})\n",
    "        self.train_writer.add_summary(r[0], self.step_idx)\n",
    "        self.step_idx += 1\n",
    "        return r[2:]\n",
    "        \n",
    "    def predict(self, input_states):\n",
    "        return self.sess.run([self.action_prob, self.value], feed_dict={self.input_states: input_states})\n",
    "    \n",
    "a2c_model = A2C(num_actions=num_actions, optimizer=tf.train.AdamOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoomEnv():\n",
    "    def __init__(self, show_window=False):\n",
    "        # Setup DoomGame\n",
    "        self.game = vizdoom.DoomGame()\n",
    "        #self.game.load_config(\"doom/my_way_home.cfg\")\n",
    "        #self.game.load_config(\"doom/defend_the_center.cfg\")\n",
    "        self.game.load_config(\"doom/basic.cfg\")\n",
    "\n",
    "        # Visualize the game (set to False to train faster)\n",
    "        self.game.set_window_visible(show_window)\n",
    "\n",
    "        # Set screen format to greyscale, improves training time\n",
    "        self.game.set_screen_format(vizdoom.ScreenFormat.GRAY8)\n",
    "\n",
    "        # Make the game end after 2100 ticks (set to 0 to disable)\n",
    "        #self.game.set_episode_timeout(2100)\n",
    "\n",
    "        # Init game\n",
    "        self.game.init()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.game.new_episode()\n",
    "        \n",
    "        # Setup initial state\n",
    "        self.frame_stack = deque(maxlen=frame_stack_size)\n",
    "        initial_frame = preprocess_frame(self.game.get_state().screen_buffer)\n",
    "        for _ in range(frame_stack_size):\n",
    "            self.frame_stack.append(initial_frame)\n",
    "        self.state = np.stack(self.frame_stack, axis=2)\n",
    "        \n",
    "envs = [DoomEnv(i == 0) for i in range(num_envs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Episode 3/500 --\n",
      "Learning rate: 0.0001\n",
      "Episode policy loss: -143.98721891641617\n",
      "Episode value loss: 5713.306781768799\n",
      "Episode entropy loss: 96.56625628471375\n",
      "Episode loss: 2712.5696226358414\n"
     ]
    }
   ],
   "source": [
    "def preprocess_frame(frame):\n",
    "    cropped_frame = frame[30:-10, 30:-30] # Crop the screen\n",
    "    normalized_frame = cropped_frame / 255.0 # Normalize Pixel Values\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [84, 84]) # Resize\n",
    "    return preprocessed_frame\n",
    "\n",
    "def calculate_expected_return(rewards, gamma):\n",
    "    expected_return = []\n",
    "    r = 0\n",
    "    for reward in rewards[::-1]: # for rewards from end to start\n",
    "        r = reward + gamma * r\n",
    "        expected_return.append(r)\n",
    "    return expected_return[::-1] # reverse so that we get the expected return from start to end\n",
    "\n",
    "episode_loss = episode_policy_loss = episode_value_loss = episode_entropy_loss = float(\"nan\")\n",
    "for episode in range(num_episodes):\n",
    "    clear_output(wait=True)\n",
    "    print(\"-- Episode {}/{} --\".format(episode, num_episodes))\n",
    "    print(\"Learning rate:\", a2c_model.learning_rate)\n",
    "    print(\"Episode policy loss:\", episode_policy_loss)\n",
    "    print(\"Episode value loss:\", episode_value_loss)\n",
    "    print(\"Episode entropy loss:\", episode_entropy_loss)\n",
    "    print(\"Episode loss:\", episode_loss)\n",
    "    for env in envs:\n",
    "        env.reset()\n",
    "    \n",
    "    # While there are running environments\n",
    "    episode_loss = episode_policy_loss = episode_value_loss = episode_entropy_loss = 0\n",
    "    while sum([env.game.is_episode_finished() for env in envs]) < num_envs:\n",
    "        states, actions, returns = [], [], []\n",
    "        \n",
    "        # For every environment\n",
    "        for env in envs:\n",
    "            # Simulate game for some number of steps\n",
    "            rewards = []\n",
    "            for _ in range(t_max):\n",
    "                # Predict and value action given state\n",
    "                # π(a_t | s_t; θ)\n",
    "                action_prob = np.squeeze(a2c_model.predict(np.expand_dims(env.state, axis=0))[0])\n",
    "                \n",
    "                # Take action stochastically \n",
    "                action = np.random.choice(np.arange(0, num_actions), p=action_prob)\n",
    "                action_one_hot = [False] * num_actions\n",
    "                action_one_hot[action] = True\n",
    "                reward = env.game.make_action(action_one_hot)\n",
    "                \n",
    "                # Store state, action and reward\n",
    "                states.append(env.state)\n",
    "                actions.append(action)\n",
    "                #actions.append(action_one_hot)\n",
    "                rewards.append(reward)\n",
    "\n",
    "                if env.game.is_episode_finished():\n",
    "                    break\n",
    "                    \n",
    "                # Get new state\n",
    "                env.frame_stack.append(preprocess_frame(env.game.get_state().screen_buffer))\n",
    "                env.state = np.stack(env.frame_stack, axis=2)\n",
    "                \n",
    "            # Calculate return (discounted rewards over a trajectory)\n",
    "            last_value = 0 if env.game.is_episode_finished() else \\\n",
    "                         a2c_model.predict(np.expand_dims(env.state, axis=0))[1][0][0]\n",
    "            returns.extend(calculate_expected_return(rewards+[last_value], discount_factor)[:-1])\n",
    "            \n",
    "        eploss, pgloss, vloss, entloss = a2c_model.train(states, actions, returns, None)\n",
    "        episode_loss         += eploss\n",
    "        episode_policy_loss  += pgloss\n",
    "        episode_value_loss   += vloss\n",
    "        episode_entropy_loss += entloss\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy = True\n",
    "for episode in range(10):\n",
    "    env = envs[0]\n",
    "    env.reset()\n",
    "    while not env.game.is_episode_finished():\n",
    "        # Predict action given state: π(a_t | s_t; θ)\n",
    "        action_prob = np.squeeze(a2c_model.predict_action(np.expand_dims(env.state, axis=0))[0])\n",
    "        if greedy:\n",
    "            action = np.argmax(action_prob)\n",
    "        else:\n",
    "            action = np.random.choice(np.arange(0, num_actions), p=action_prob) # Sample action stochastically\n",
    "        action_one_hot = [False] * num_actions\n",
    "        action_one_hot[action] = True\n",
    "\n",
    "        # Take the action\n",
    "        env.game.make_action(action_one_hot)\n",
    "        \n",
    "        if not env.game.is_episode_finished():\n",
    "            # Get new state\n",
    "            env.frame_stack.append(preprocess_frame(env.game.get_state().screen_buffer))\n",
    "            env.state = np.stack(env.frame_stack, axis=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
