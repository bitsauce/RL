{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import vizdoom\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from skimage import transform\n",
    "from IPython.display import display, clear_output\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduler():\n",
    "    def __init__(self, initial_value, interval, decay_factor):\n",
    "        self.interval = self.counter = interval\n",
    "        self.decay_factor = decay_factor\n",
    "        self.value_factor = 1\n",
    "        self.value = initial_value\n",
    "        \n",
    "    def get_value(self):\n",
    "        self.counter -= 1\n",
    "        if self.counter < 0:\n",
    "            self.counter = self.interval\n",
    "            self.value *= self.decay_factor\n",
    "        return self.value\n",
    "        \n",
    "lr_scheduler = Scheduler(initial_value=1e-2, interval=10, decay_factor=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoomEnv():\n",
    "    def __init__(self, show_window=False):\n",
    "        # Setup DoomGame\n",
    "        self.game = vizdoom.DoomGame()\n",
    "        self.game.load_config(\"doom/my_way_home.cfg\")\n",
    "\n",
    "        # Visualize the game (set to False to train faster)\n",
    "        self.game.set_window_visible(show_window)\n",
    "\n",
    "        # Set screen format to greyscale, improves training time\n",
    "        self.game.set_screen_format(vizdoom.ScreenFormat.GRAY8)\n",
    "\n",
    "        # Make the game end after 2100 ticks (set to 0 to disable)\n",
    "        #self.game.set_episode_timeout(2100)\n",
    "        \n",
    "        self.game.set_available_game_variables([vizdoom.GameVariable.VELOCITY_X, vizdoom.GameVariable.VELOCITY_Y])\n",
    "\n",
    "        # Init game\n",
    "        self.game.init()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.game.new_episode()\n",
    "        \n",
    "        # Setup initial state\n",
    "        self.frame_stack = deque(maxlen=frame_stack_size)\n",
    "        initial_frame = preprocess_frame(self.game.get_state().screen_buffer)\n",
    "        for _ in range(frame_stack_size):\n",
    "            self.frame_stack.append(initial_frame)\n",
    "        self.state = np.stack(self.frame_stack, axis=2)        \n",
    "        self.total_reward = 0\n",
    "\n",
    "num_envs = 8\n",
    "envs = [DoomEnv(i == 0) for i in range(num_envs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num actions: 5\n"
     ]
    }
   ],
   "source": [
    "discount_factor  = 0.99\n",
    "num_actions      = envs[0].game.get_available_buttons_size()\n",
    "save_interval    = 100\n",
    "t_max            = 50\n",
    "frame_stack_size = 4\n",
    "print(\"Num actions:\", num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(logits):\n",
    "    a0 = logits - tf.reduce_max(logits, axis=-1, keepdims=True)\n",
    "    ea0 = tf.exp(a0)\n",
    "    z0 = tf.reduce_sum(ea0, axis=-1, keepdims=True)\n",
    "    p0 = ea0 / z0\n",
    "    return tf.reduce_sum(p0 * (tf.log(z0) - a0), axis=-1)\n",
    "\n",
    "class A2C():\n",
    "    def __init__(self, num_actions, optimizer, value_scale=0.5, entropy_scale=0.01, model_checkpoint=None):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Construct model\n",
    "        self.input_states = tf.placeholder(shape=(None, 84, 84, 4), dtype=tf.float32)\n",
    "        self.input_velocity = tf.placeholder(shape=(None, 2), dtype=tf.float32)\n",
    "        self.conv1  = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"valid\")(self.input_states)\n",
    "        self.pool1  = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(self.conv1)\n",
    "        self.conv2  = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"valid\")(self.pool1)\n",
    "        self.pool2  = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(self.conv2)\n",
    "        self.conv3  = tf.keras.layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"valid\")(self.pool2)\n",
    "        self.flatten = tf.keras.layers.Flatten()(self.conv3)\n",
    "        self.dense1 = tf.keras.layers.Dense(256, activation=\"relu\")(self.flatten)\n",
    "        self.shared_features = tf.concat([self.dense1, self.input_velocity], axis=-1)\n",
    "        \n",
    "        # Policy branch π(a_t | s_t; θ)\n",
    "        self.action_logits = tf.keras.layers.Dense(num_actions, activation=None)(self.shared_features)\n",
    "        self.action_prob   = tf.keras.layers.Softmax()(self.action_logits)\n",
    "        \n",
    "        # Value branch V(s_t; θ)\n",
    "        self.value = tf.keras.layers.Dense(1, activation=\"relu\")(self.shared_features)\n",
    "        \n",
    "        # Create policy gradient train function\n",
    "        self.actions_placeholder = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "        self.returns_placeholder = tf.placeholder(shape=(None,), dtype=tf.float32)\n",
    "        self.values_placeholder  = tf.placeholder(shape=(None,), dtype=tf.float32)\n",
    "        self.lr_placeholder      = tf.placeholder(shape=(), dtype=tf.float32)\n",
    "        \n",
    "        # Get probabilities of taken actions: log π(a_t | s_t; θ)\n",
    "        # log_action_prob = tf.log(tf.reduce_sum(self.action_prob * self.actions_onehot_placeholder, axis=1))\n",
    "        self.neg_log_action = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.action_logits,\n",
    "                                                                             labels=self.actions_placeholder)\n",
    "        \n",
    "        # Policy Gradient Loss = ∇_θ log π(a_t | s_t; θ)(R_t − V(s_t; θ_v))\n",
    "        # Negative log likelihood of the taken actions, weighted by the discounted and normalized rewards\n",
    "        # self.policy_loss = -tf.reduce_mean(log_action_prob * (self.returns_placeholder - self.value))\n",
    "        self.policy_loss  = tf.reduce_mean((self.returns_placeholder - self.values_placeholder) * self.neg_log_action)\n",
    "        \n",
    "        # Get value loss\n",
    "        # MSE(V(s_t), R_t)\n",
    "        self.value_loss = tf.reduce_mean(tf.squared_difference(tf.squeeze(self.value), self.returns_placeholder))\n",
    "        \n",
    "        # Get entropy\n",
    "        # self.entropy_loss = -tf.reduce_mean(self.action_prob * tf.log(self.action_prob + 0.0001))\n",
    "        self.entropy_loss = tf.reduce_mean(entropy(self.action_logits))\n",
    "        \n",
    "        # Total loss\n",
    "        self.loss = self.policy_loss + self.value_loss * value_scale - self.entropy_loss * entropy_scale\n",
    "        \n",
    "        # Minimize loss\n",
    "        self.optimizer = optimizer(learning_rate=self.lr_placeholder, decay=0.99)\n",
    "        self.learning_rate = 1e-4\n",
    "        self.train_step = self.optimizer.minimize(self.loss)\n",
    "        \n",
    "        # Create session\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "        # Run the initializer\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        tf.summary.scalar(\"policy_loss\", self.policy_loss)\n",
    "        tf.summary.scalar(\"value_loss\", self.value_loss)\n",
    "        tf.summary.scalar(\"entropy_loss\", self.entropy_loss)\n",
    "        tf.summary.scalar(\"loss\", self.loss)\n",
    "        tf.summary.scalar(\"learning_rate\", self.lr_placeholder)\n",
    "        self.summary_merged = tf.summary.merge_all()\n",
    "        \n",
    "        # Load model checkpoint if provided\n",
    "        self.saver = tf.train.Saver()\n",
    "        if model_checkpoint:\n",
    "            self.run_idx = int(re.findall(r\"_run\\d+\", model_checkpoint)[0][len(\"_run\"):])\n",
    "            self.step_idx = int(re.findall(r\"_step\\d+\", model_checkpoint)[0][len(\"_step\"):])\n",
    "            self.saver.restore(self.sess, model_checkpoint)\n",
    "            print(\"Model checkpoint restored from {}\".format(model_checkpoint))\n",
    "        else:\n",
    "            self.run_idx = 0\n",
    "            while os.path.isdir(\"./logs/run{}\".format(self.run_idx)):\n",
    "                self.run_idx += 1\n",
    "            self.step_idx = 0\n",
    "        self.train_writer = tf.summary.FileWriter(\"./logs/run{}\".format(self.run_idx), self.sess.graph)\n",
    "        \n",
    "        \n",
    "    def save(self):\n",
    "        model_checkpoint = \"./models/a2c_run{}_step{}.ckpt\".format(self.run_idx, self.step_idx)\n",
    "        self.saver.save(self.sess, model_checkpoint)\n",
    "        print(\"Model checkpoint saved to {}\".format(model_checkpoint))\n",
    "        \n",
    "    def train(self, input_states, actions, returns, values, aux_state_vars):\n",
    "        r = self.sess.run([self.summary_merged, self.train_step, self.loss,\n",
    "                           self.policy_loss, self.value_loss, self.entropy_loss],\n",
    "                          feed_dict={self.input_states: input_states,\n",
    "                                     self.actions_placeholder: actions,\n",
    "                                     self.returns_placeholder: returns,\n",
    "                                     self.values_placeholder: values,\n",
    "                                     self.lr_placeholder: self.learning_rate,\n",
    "                                     self.input_velocity: aux_state_vars})\n",
    "        self.train_writer.add_summary(r[0], self.step_idx)\n",
    "        self.step_idx += 1\n",
    "        return r[2:]\n",
    "        \n",
    "    def predict(self, input_states, aux_state_vars):\n",
    "        return self.sess.run([self.action_prob, self.value], feed_dict={self.input_states: input_states, self.input_velocity: aux_state_vars})\n",
    "    \n",
    "a2c_model = A2C(num_actions=num_actions, optimizer=tf.train.RMSPropOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Episode 9 --\n",
      "Learning rate: 0.01\n",
      "Episode policy loss: -0.14821111061610281\n",
      "Episode value loss: 0.0002886151510210766\n",
      "Episode entropy loss: 67.46145021915436\n",
      "Episode loss: -0.8226812947541475\n",
      "Average episode reward: -0.20999999999999325\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting envronments...\n",
      "Training...\n"
     ]
    }
   ],
   "source": [
    "def preprocess_frame(frame):\n",
    "    cropped_frame = frame[30:-10, 30:-30] # Crop the screen\n",
    "    normalized_frame = cropped_frame / 255.0 # Normalize Pixel Values\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [84, 84]) # Resize\n",
    "    return preprocessed_frame\n",
    "\n",
    "def calculate_expected_return(rewards, gamma):\n",
    "    expected_return = []\n",
    "    r = 0\n",
    "    for reward in rewards[::-1]: # for rewards from end to start\n",
    "        r = reward + gamma * r\n",
    "        expected_return.append(r)\n",
    "    return expected_return[::-1] # reverse so that we get the expected return from start to end\n",
    "\n",
    "average_episode_rewards = []\n",
    "#for episode in range(num_episodes):\n",
    "episode = 0\n",
    "while True:\n",
    "    print(\"Resetting envronments...\")\n",
    "    episode += 1\n",
    "    for env in envs:\n",
    "        env.reset()\n",
    "    \n",
    "    # While there are running environments\n",
    "    print(\"Training...\")\n",
    "    a2c_model.learning_rate = lr_scheduler.get_value()\n",
    "    episode_loss = episode_policy_loss = episode_value_loss = episode_entropy_loss = 0\n",
    "    average_episode_reward = []\n",
    "    while sum([env.game.is_episode_finished() for env in envs]) < num_envs:\n",
    "        states, actions, returns, values, auxiliary_state_vars = [], [], [], [], []\n",
    "        \n",
    "        # For every environment\n",
    "        for env in envs:\n",
    "            # Simulate game for some number of steps\n",
    "            rewards = []\n",
    "            for _ in range(t_max):\n",
    "                \n",
    "                velocity = [env.game.get_game_variable(vizdoom.GameVariable.VELOCITY_X),\n",
    "                            env.game.get_game_variable(vizdoom.GameVariable.VELOCITY_Y)]\n",
    "                \n",
    "                # Predict and value action given state\n",
    "                # π(a_t | s_t; θ)\n",
    "                action_prob, value = a2c_model.predict(np.expand_dims(env.state, axis=0), np.expand_dims(velocity, axis=0))\n",
    "                action_prob, value = np.squeeze(action_prob), np.squeeze(value)\n",
    "                \n",
    "                # Take action stochastically \n",
    "                action = np.random.choice(np.arange(0, num_actions), p=action_prob)\n",
    "                action_one_hot = [False] * num_actions\n",
    "                action_one_hot[action] = True\n",
    "                reward = env.game.make_action(action_one_hot)\n",
    "                velocity = [env.game.get_game_variable(vizdoom.GameVariable.VELOCITY_X),\n",
    "                            env.game.get_game_variable(vizdoom.GameVariable.VELOCITY_Y)]\n",
    "                auxiliary_state_vars.append(velocity)\n",
    "                \n",
    "                # Store state, action and reward\n",
    "                states.append(env.state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                values.append(value)\n",
    "                env.total_reward += reward\n",
    "\n",
    "                if env.game.is_episode_finished():\n",
    "                    break\n",
    "                    \n",
    "                # Get new state\n",
    "                env.frame_stack.append(preprocess_frame(env.game.get_state().screen_buffer))\n",
    "                env.state = np.stack(env.frame_stack, axis=2)\n",
    "                \n",
    "            # Calculate return (discounted rewards over a trajectory)\n",
    "            last_value = 0 if env.game.is_episode_finished() else \\\n",
    "                         a2c_model.predict(np.expand_dims(env.state, axis=0), np.expand_dims(velocity, axis=0))[1][0][0]\n",
    "            returns.extend(calculate_expected_return(rewards+[last_value], discount_factor)[:-1])\n",
    "            average_episode_reward.extend(rewards)\n",
    "            \n",
    "        eploss, pgloss, vloss, entloss = a2c_model.train(states, actions, returns, values, auxiliary_state_vars)\n",
    "        episode_loss         += eploss\n",
    "        episode_policy_loss  += pgloss\n",
    "        episode_value_loss   += vloss\n",
    "        episode_entropy_loss += entloss\n",
    "    average_episode_rewards.append(sum([env.total_reward for env in envs]) / len(envs))\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(\"-- Episode {} --\".format(episode))\n",
    "    print(\"Learning rate:\", a2c_model.learning_rate)\n",
    "    print(\"Episode policy loss:\", episode_policy_loss)\n",
    "    print(\"Episode value loss:\", episode_value_loss)\n",
    "    print(\"Episode entropy loss:\", episode_entropy_loss)\n",
    "    print(\"Episode loss:\", episode_loss)\n",
    "    print(\"Average episode reward:\", average_episode_rewards[-1])\n",
    "    print(\"\")\n",
    "    plt.plot(np.arange(0, len(average_episode_rewards)), average_episode_rewards)\n",
    "    plt.show()\n",
    "    \n",
    "    if episode % save_interval == 0:\n",
    "        a2c_model.save()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "env = DoomEnv(True)\n",
    "greedy = True\n",
    "for episode in range(10):\n",
    "    env.reset()\n",
    "    while not env.game.is_episode_finished():\n",
    "        # Predict action given state: π(a_t | s_t; θ)\n",
    "        action_prob = np.squeeze(a2c_model.predict(np.expand_dims(env.state, axis=0))[0])\n",
    "        if greedy:\n",
    "            action = np.argmax(action_prob)\n",
    "        else:\n",
    "            action = np.random.choice(np.arange(0, num_actions), p=action_prob) # Sample action stochastically\n",
    "        action_one_hot = [False] * num_actions\n",
    "        action_one_hot[action] = True\n",
    "\n",
    "        # Take the action\n",
    "        env.game.make_action(action_one_hot)\n",
    "        time.sleep(0.016)\n",
    "        \n",
    "        if not env.game.is_episode_finished():\n",
    "            # Get new state\n",
    "            env.frame_stack.append(preprocess_frame(env.game.get_state().screen_buffer))\n",
    "            env.state = np.stack(env.frame_stack, axis=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
